{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import time\n",
    "start_time = time.time()\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "import numpy as np\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.linear_model import (LinearRegression, SGDRegressor)\n",
    "import lightgbm as lgb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.linear_model import (LinearRegression, SGDRegressor)\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "Validation = False\n",
    "reduce_size = False\n",
    "\n",
    "seed = 0\n",
    "\n",
    "# Data path\n",
    "data_path = '../readonly/final_project_data/'\n",
    "submission_path = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<module 'builtins' (built-in)>,\n",
       " <module 'time' (built-in)>,\n",
       " <module 'numpy' from '/home/zed/miniconda3/envs/courseproj/lib/python3.5/site-packages/numpy/__init__.py'>,\n",
       " <module 'warnings' from '/home/zed/miniconda3/envs/courseproj/lib/python3.5/warnings.py'>,\n",
       " <module 'pandas' from '/home/zed/miniconda3/envs/courseproj/lib/python3.5/site-packages/pandas/__init__.py'>,\n",
       " <module 'builtins' (built-in)>,\n",
       " <module 'lightgbm' from '/home/zed/miniconda3/envs/courseproj/lib/python3.5/site-packages/lightgbm/__init__.py'>,\n",
       " <module 'pip' from '/home/zed/miniconda3/envs/courseproj/lib/python3.5/site-packages/pip/__init__.py'>,\n",
       " <module 'gc' (built-in)>,\n",
       " <module 'sklearn.preprocessing' from '/home/zed/miniconda3/envs/courseproj/lib/python3.5/site-packages/sklearn/preprocessing/__init__.py'>,\n",
       " <module 'types' from '/home/zed/miniconda3/envs/courseproj/lib/python3.5/types.py'>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import types\n",
    "def imports():\n",
    "    for name, val in globals().items():\n",
    "        if isinstance(val, types.ModuleType):\n",
    "            yield val\n",
    "list(imports())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downcast_dtypes(df):\n",
    "    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
    "    int_cols =   [c for c in df if df[c].dtype in [\"int64\", \"int32\"]]\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols]   = df[int_cols].astype(np.int16)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02 min: Start loading data\n"
     ]
    }
   ],
   "source": [
    "print('%0.2f min: Start loading data'%((time.time() - start_time)/60))\n",
    "\n",
    "sale_train = pd.read_csv('%s/sales_train.csv.gz' % data_path)\n",
    "test  = pd.read_csv('%s/test.csv.gz' % data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strange Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at line 2909818, very low item_price and high item_cnt_day\n",
    "sale_train[sale_train['item_id'] == 11373].sort_values(['item_price'])\n",
    "\n",
    "# Look at line 885138, very high item_price\n",
    "sale_train[sale_train['item_id'] == 11365].sort_values(['item_price'])\n",
    "\n",
    "# Correct sale_train values\n",
    "\n",
    "# Replace with median\n",
    "sale_train['item_price'][2909818] = np.nan\n",
    "\n",
    "sale_train['item_cnt_day'][2909818] = np.nan\n",
    "\n",
    "sale_train['item_price'][2909818] = sale_train[(sale_train['shop_id'] ==12) & (sale_train['item_id'] == 11373) & (sale_train['date_block_num'] == 33)]['item_price'].median()\n",
    "\n",
    "sale_train['item_cnt_day'][2909818] = round(sale_train[(sale_train['shop_id'] ==12) & (sale_train['item_id'] == 11373) & (sale_train['date_block_num'] == 33)]['item_cnt_day'].median())\n",
    "\n",
    "sale_train['item_price'][885138] = np.nan\n",
    "\n",
    "sale_train['item_price'][885138] = sale_train[(sale_train['item_id'] == 11365) & (sale_train['shop_id'] ==12) & (sale_train['date_block_num'] == 8)]['item_price'].median()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12 min: Finish loading data\n"
     ]
    }
   ],
   "source": [
    "# Consider only shops in test set\n",
    "test_nrow = test.shape[0]\n",
    "\n",
    "sale_train = sale_train.merge(test[['shop_id']].drop_duplicates(), how = 'inner')\n",
    "sale_train['date'] = pd.to_datetime(sale_train['date'], format = '%d.%m.%Y')\n",
    "\n",
    "print('%0.2f min: Finish loading data'%((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17 min: Finish creating the grid\n"
     ]
    }
   ],
   "source": [
    "# For every month we create a grid from all shops/items combinations from that month\n",
    "\n",
    "grid = []\n",
    "for block_num in sale_train['date_block_num'].unique():\n",
    "    cur_shops = sale_train[sale_train['date_block_num']==block_num]['shop_id'].unique()\n",
    "    cur_items = sale_train[sale_train['date_block_num']==block_num]['item_id'].unique()\n",
    "    # All combinations\n",
    "    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n",
    "\n",
    "#Turn the grid into pandas dataframe\n",
    "index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n",
    "\n",
    "print('%0.2f min: Finish creating the grid'%((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24 min: Finish joining gb_cnt\n",
      "2934456.0\n",
      "2671279\n",
      "2671279\n"
     ]
    }
   ],
   "source": [
    "index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "\n",
    "# Clip the label between 0 and 20\n",
    "sale_train['item_cnt_day'] = sale_train['item_cnt_day'].clip(0,20)\n",
    "# Group the number of items sold by month rather than by day\n",
    "gb_cnt = sale_train.groupby(index_cols)['item_cnt_day'].agg(['sum']).reset_index().rename(columns = {'sum': 'item_cnt_month'})\n",
    "\n",
    "gb_cnt['item_cnt_month'] = gb_cnt['item_cnt_month'].clip(0,20).astype(np.uint8)\n",
    "\n",
    "#Join aggregated data to the grid\n",
    "train = pd.merge(grid,gb_cnt,how='left',on=index_cols).fillna(0)\n",
    "train['item_cnt_month'] = train['item_cnt_month'].astype(np.uint8)\n",
    "train = downcast_dtypes(train)\n",
    "\n",
    "#Sort the data\n",
    "train.sort_values(['date_block_num','shop_id','item_id'],inplace=True)\n",
    "print('%0.2f min: Finish joining gb_cnt'%((time.time() - start_time)/60))\n",
    "\n",
    "# Sanity check\n",
    "print(sale_train['item_cnt_day'].sum())\n",
    "print(train['item_cnt_month'].sum())\n",
    "print(gb_cnt['item_cnt_month'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26 min: Finish adding item_category_id\n"
     ]
    }
   ],
   "source": [
    "# Merge items with train\n",
    "\n",
    "item = pd.read_csv('%s/items.csv' % data_path)\n",
    "\n",
    "train = train.merge(item[['item_id', 'item_category_id']], on = ['item_id'], how = 'left')\n",
    "test = test.merge(item[['item_id', 'item_category_id']], on = ['item_id'], how = 'left')\n",
    "\n",
    "print('%0.2f min: Finish adding item_category_id'%((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate item categories from Russian\n",
    "item_cat = pd.read_csv('%s/item_categories.csv' % data_path)\n",
    "\n",
    "l_cat = list(item_cat.item_category_name)\n",
    "for ind in range(0,1):\n",
    "    l_cat[ind] = 'PC Headsets / Headphones'\n",
    "\n",
    "for ind in range(1,8):\n",
    "    l_cat[ind] = 'Access'\n",
    "    \n",
    "l_cat[8] = 'Tickets (figure)'\n",
    "l_cat[9] = 'Delivery of goods'\n",
    "\n",
    "for ind in range(10,18):\n",
    "    l_cat[ind] = 'Consoles'\n",
    "\n",
    "for ind in range(18,25):\n",
    "    l_cat[ind] = 'Consoles Games'\n",
    "\n",
    "l_cat[25] = 'Accessories for games'\n",
    "\n",
    "for ind in range(26,28):\n",
    "    l_cat[ind] = 'phone games'\n",
    "\n",
    "for ind in range(28,32):\n",
    "    l_cat[ind] = 'CD games'\n",
    "\n",
    "for ind in range(32,37):\n",
    "    l_cat[ind] = 'Card'\n",
    "\n",
    "for ind in range(37,43):\n",
    "    l_cat[ind] = 'Movie'\n",
    "\n",
    "for ind in range(43,55):\n",
    "    l_cat[ind] = 'Books'\n",
    "\n",
    "for ind in range(55,61):\n",
    "    l_cat[ind] = 'Music'\n",
    "\n",
    "for ind in range(61,73):\n",
    "    l_cat[ind] = 'Gifts'\n",
    "\n",
    "for ind in range(73,79):\n",
    "    l_cat[ind] = 'Soft'\n",
    "\n",
    "for ind in range(79,81):\n",
    "    l_cat[ind] = 'Office'\n",
    "\n",
    "for ind in range(81,83):\n",
    "    l_cat[ind] = 'Clean'\n",
    "\n",
    "l_cat[83] = 'Elements of a food'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27 min: Finish adding item_cat_id_fix\n"
     ]
    }
   ],
   "source": [
    "# Merge item category encoding with train\n",
    "lb = preprocessing.LabelEncoder()\n",
    "item_cat['item_cat_id_fix'] = lb.fit_transform(l_cat)\n",
    "\n",
    "train = train.merge(item_cat[['item_cat_id_fix', 'item_category_id']], on = ['item_category_id'], how = 'left')\n",
    "test = test.merge(item_cat[['item_cat_id_fix', 'item_category_id']], on = ['item_category_id'], how = 'left')\n",
    "\n",
    "del item, item_cat, grid, gb_cnt\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print('%0.2f min: Finish adding item_cat_id_fix'%((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27 min: Start adding mean-encoding for item_cnt_month\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 1/4 [00:07<00:23,  7.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       Cor\n",
      "shop_id_cnt_month_mean_Kfold      0.173370\n",
      "shop_id_target_mean_LOO           0.175547\n",
      "shop_id_cnt_month_mean_Smooth     0.175572\n",
      "shop_id_cnt_month_mean_Expanding  0.175746\n",
      "0.40 min: Finish encoding shop_id\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 2/4 [00:17<00:16,  8.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       Cor\n",
      "item_id_cnt_month_mean_Kfold      0.315862\n",
      "item_id_cnt_month_mean_Smooth     0.479840\n",
      "item_id_target_mean_LOO           0.481937\n",
      "item_id_cnt_month_mean_Expanding  0.565646\n",
      "0.55 min: Finish encoding item_id\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 3/4 [00:25<00:08,  8.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Cor\n",
      "item_category_id_cnt_month_mean_Kfold      0.274072\n",
      "item_category_id_cnt_month_mean_Smooth     0.292732\n",
      "item_category_id_target_mean_LOO           0.292778\n",
      "item_category_id_cnt_month_mean_Expanding  0.296104\n",
      "0.70 min: Finish encoding item_category_id\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:34<00:00,  8.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Cor\n",
      "item_cat_id_fix_cnt_month_mean_Kfold      0.157323\n",
      "item_cat_id_fix_target_mean_LOO           0.171593\n",
      "item_cat_id_fix_cnt_month_mean_Smooth     0.171639\n",
      "item_cat_id_fix_cnt_month_mean_Expanding  0.176845\n",
      "0.84 min: Finish encoding item_cat_id_fix\n",
      "0.84 min: Finish adding mean-encoding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# For Trainset\n",
    "print('%0.2f min: Start adding mean-encoding for item_cnt_month'%((time.time() - start_time)/60))\n",
    "\n",
    "target = 'item_cnt_month'\n",
    "global_mean =  train[target].mean()\n",
    "y_tr = train[target].values\n",
    "\n",
    "mean_encoded_col = ['shop_id', 'item_id', 'item_category_id', 'item_cat_id_fix']\n",
    "\n",
    "for col in tqdm(mean_encoded_col):\n",
    "    col_tr = train[[col] + [target]]\n",
    "    corrcoefs = pd.DataFrame(columns = ['Cor'])\n",
    "\n",
    "    # Mean encodings - KFold scheme\n",
    "    kf = KFold(n_splits = 5, shuffle = False, random_state = seed)\n",
    "    col_tr[col + '_cnt_month_mean_Kfold'] = global_mean\n",
    "\n",
    "    for tr_ind, val_ind in kf.split(col_tr):\n",
    "        # Identify train and test rows based on indexes\n",
    "        X_tr, X_val = col_tr.iloc[tr_ind], col_tr.iloc[val_ind]\n",
    "        # Calculate mean and save it\n",
    "        means = X_val[col].map(X_tr.groupby(col)[target].mean())\n",
    "        X_val[col + '_cnt_month_mean_Kfold'] = means\n",
    "        col_tr.iloc[val_ind] = X_val\n",
    "        \n",
    "    col_tr.fillna(global_mean, inplace = True)\n",
    "\n",
    "    corrcoefs.loc[col + '_cnt_month_mean_Kfold'] = np.corrcoef(y_tr, col_tr[col + '_cnt_month_mean_Kfold'])[0][1]\n",
    "    \n",
    "    # Mean encodings - Leave-one-out scheme\n",
    "\n",
    "    item_id_target_sum = col_tr.groupby(col)[target].sum()\n",
    "    item_id_target_count = col_tr.groupby(col)[target].count()\n",
    "\n",
    "    col_tr[col + '_cnt_month_sum'] = col_tr[col].map(item_id_target_sum)\n",
    "    col_tr[col + '_cnt_month_count'] = col_tr[col].map(item_id_target_count)\n",
    "    col_tr[col + '_target_mean_LOO'] = (col_tr[col + '_cnt_month_sum'] - col_tr[target]) / (col_tr[col + '_cnt_month_count'] - 1)\n",
    "\n",
    "    col_tr.fillna(global_mean, inplace = True)\n",
    "    corrcoefs.loc[col + '_target_mean_LOO'] = np.corrcoef(y_tr, col_tr[col + '_target_mean_LOO'])[0][1]\n",
    "    \n",
    "    # Mean encodings - Smoothing\n",
    "\n",
    "    item_id_target_mean = col_tr.groupby(col)[target].mean()\n",
    "    item_id_target_count = col_tr.groupby(col)[target].count()\n",
    "    \n",
    "    col_tr[col + '_cnt_month_mean'] = col_tr[col].map(item_id_target_mean)\n",
    "    col_tr[col + '_cnt_month_count'] = col_tr[col].map(item_id_target_count)\n",
    "\n",
    "    alpha = 100\n",
    "    col_tr[col + '_cnt_month_mean_Smooth'] = (col_tr[col + '_cnt_month_mean'] *  col_tr[col + '_cnt_month_count'] + global_mean * alpha) / (alpha + col_tr[col + '_cnt_month_count'])\n",
    "    col_tr[col + '_cnt_month_mean_Smooth'].fillna(global_mean, inplace=True)\n",
    "    corrcoefs.loc[col + '_cnt_month_mean_Smooth'] = np.corrcoef(y_tr, col_tr[col + '_cnt_month_mean_Smooth'])[0][1]\n",
    "    \n",
    "    # Mean encodings - Expanding mean scheme\n",
    "\n",
    "    cumsum = col_tr.groupby(col)[target].cumsum() - col_tr[target]\n",
    "    sumcnt = col_tr.groupby(col).cumcount()\n",
    "\n",
    "    col_tr[col + '_cnt_month_mean_Expanding'] = cumsum / sumcnt\n",
    "    col_tr[col + '_cnt_month_mean_Expanding'].fillna(global_mean, inplace=True)\n",
    "    corrcoefs.loc[col + '_cnt_month_mean_Expanding'] = np.corrcoef(y_tr, col_tr[col + '_cnt_month_mean_Expanding'])[0][1]\n",
    "\n",
    "    # Add the best encoding out of the 4 to the train set\n",
    "    train = pd.concat([train, col_tr[corrcoefs['Cor'].idxmax()]], axis = 1)\n",
    "\n",
    "    print(corrcoefs.sort_values('Cor'))\n",
    "    print('%0.2f min: Finish encoding %s'%((time.time() - start_time)/60, col))\n",
    "\n",
    "print('%0.2f min: Finish adding mean-encoding'%((time.time() - start_time)/60))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.84 min: Start combining data\n"
     ]
    }
   ],
   "source": [
    "# Combine trainset and testset \n",
    "print('%0.2f min: Start combining data'%((time.time() - start_time)/60))\n",
    "\n",
    "# If I do not use a val set, I combine month 34 with the train set\n",
    "if Validation == False:\n",
    "    test['date_block_num'] = 34\n",
    "    all_data = pd.concat([train, test], axis = 0)\n",
    "    all_data = all_data.drop(columns = ['ID'])\n",
    "\n",
    "else:\n",
    "    all_data = train\n",
    "\n",
    "del train, test, col_tr\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "all_data = downcast_dtypes(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lag based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.87 min: Start adding lag-based feature\n",
      "Features that will be shifted:\n",
      "['item_cat_id_fix_cnt_month_mean_Expanding', 'item_category_id_cnt_month_mean_Expanding', 'item_cnt_month', 'item_id_cnt_month_mean_Expanding', 'shop_id_cnt_month_mean_Expanding']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:27<00:00,  4.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.34 min: Finish generating lag features\n"
     ]
    }
   ],
   "source": [
    "# Creating item/shop pair lags lag-based features \n",
    "\n",
    "print('%0.2f min: Start adding lag-based feature'%((time.time() - start_time)/60))\n",
    "\n",
    "index_cols = ['shop_id', 'item_id', 'item_category_id', 'item_cat_id_fix', 'date_block_num']\n",
    "cols_to_rename = list(all_data.columns.difference(index_cols))\n",
    "print('Features that will be shifted:')\n",
    "print(cols_to_rename)\n",
    "# Lag range\n",
    "shift_range = [1, 2, 3, 4, 6, 12]\n",
    "\n",
    "# this loop just adds the future months\n",
    "for month_shift in tqdm(shift_range):\n",
    "    train_shift = all_data[index_cols + cols_to_rename].copy()\n",
    "    train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n",
    "\n",
    "    foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n",
    "    # Rename the shifted feature\n",
    "    train_shift = train_shift.rename(columns=foo)\n",
    "    # the shift works because date_block num is considered as index; the block nums outside the range are not considered given the left join\n",
    "    all_data = pd.merge(all_data, train_shift, on=index_cols, how='left').fillna(0)\n",
    "\n",
    "del train_shift\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# Don't use old data from year 2013\n",
    "all_data = all_data[all_data['date_block_num'] >= 12] \n",
    "# Take all the lag columns (the ones which end with a lag number)\n",
    "lag_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]]\n",
    "\n",
    "all_data = downcast_dtypes(all_data)\n",
    "\n",
    "print('%0.2f min: Finish generating lag features'%((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.34 min: Start getting date features\n",
      "1.36 min: Finish getting date features\n"
     ]
    }
   ],
   "source": [
    "# Creating date features \n",
    "\n",
    "print('%0.2f min: Start getting date features'%((time.time() - start_time)/60))\n",
    "\n",
    "dates_train = sale_train[['date', 'date_block_num']].drop_duplicates()\n",
    "\n",
    "dates_test = dates_train[dates_train['date_block_num'] == 34-12]\n",
    "\n",
    "\n",
    "dates_test['date_block_num'] = 34\n",
    "dates_test['date'] = dates_test['date'] + pd.DateOffset(years=1)\n",
    "\n",
    "dates_all = pd.concat([dates_train, dates_test])\n",
    "\n",
    "dates_all['dow'] = dates_all['date'].dt.dayofweek\n",
    "dates_all['year'] = dates_all['date'].dt.year\n",
    "dates_all['month'] = dates_all['date'].dt.month\n",
    "dates_all = pd.get_dummies(dates_all, columns=['dow'])\n",
    "\n",
    "dow_col = ['dow_' + str(x) for x in range(7)]\n",
    "\n",
    "date_features = dates_all.groupby(['year', 'month', 'date_block_num'])[dow_col].agg('sum').reset_index()\n",
    "date_features['days_of_month'] = date_features[dow_col].sum(axis=1)\n",
    "date_features['year'] = date_features['year'] - 2013\n",
    "\n",
    "date_features = date_features[['month', 'year', 'days_of_month', 'date_block_num']]\n",
    "all_data = all_data.merge(date_features, on = 'date_block_num', how = 'left')\n",
    "date_columns = date_features.columns.difference(set(index_cols))\n",
    "\n",
    "print('%0.2f min: Finish getting date features'%((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale Feature Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.92 min: Finish scaling features\n"
     ]
    }
   ],
   "source": [
    "train = all_data[all_data['date_block_num']!= all_data['date_block_num'].max()]\n",
    "test = all_data[all_data['date_block_num']== all_data['date_block_num'].max()]\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "to_drop_cols = ['date_block_num']\n",
    "feature_columns = list(set(lag_cols + index_cols + list(date_columns)).difference(to_drop_cols))\n",
    "\n",
    "# Scale the test set based on the Scaler fitted for the train\n",
    "train[feature_columns] = sc.fit_transform(train[feature_columns])\n",
    "test[feature_columns] = sc.transform(test[feature_columns])\n",
    "\n",
    "all_data_scaled = pd.concat([train, test], axis = 0)\n",
    "all_data_scaled = downcast_dtypes(all_data)\n",
    "\n",
    "del train, test, date_features, sale_train\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print('%0.2f min: Finish scaling features'%((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First- Level Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.92 min: Start training First level models\n"
     ]
    }
   ],
   "source": [
    "# Save date_block_num, as it can't use them as features, but it will be needed to split the dataset into parts\n",
    "\n",
    "dates = all_data['date_block_num']\n",
    "last_block = dates.max()\n",
    "\n",
    "print('%0.2f min: Start training First level models'%((time.time() - start_time)/60))\n",
    "\n",
    "start_first_level_total = time.perf_counter()\n",
    "\n",
    "scoringMethod = 'r2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Start training for month27\n",
      "Training Model 0: sgdr\n",
      "SGDRegressor runs for 4.78 seconds.\n",
      "\n",
      "Training Model 1: lightgbm\n",
      "lightgbm runs for 123.80 seconds.\n",
      "\n",
      "Training Model 2: keras\n",
      "Epoch 1/5\n",
      "3746814/3746814 [==============================] - 5s 1us/step - loss: 1.6347 - mean_squared_error: 1.6347\n",
      "Epoch 2/5\n",
      "3746814/3746814 [==============================] - 2s 1us/step - loss: 1.6347 - mean_squared_error: 1.6347\n",
      "Epoch 3/5\n",
      "3746814/3746814 [==============================] - 2s 1us/step - loss: 1.6347 - mean_squared_error: 1.6347\n",
      "Epoch 4/5\n",
      "3746814/3746814 [==============================] - 2s 1us/step - loss: 1.6347 - mean_squared_error: 1.6347\n",
      "Epoch 5/5\n",
      "3746814/3746814 [==============================] - 2s 1us/step - loss: 1.6347 - mean_squared_error: 1.6347\n",
      "221482/221482 [==============================] - 0s 0us/step\n",
      "keras runs for 13.69 seconds.\n",
      "Total running time was 2.39 minutes.\n",
      "Training Model 3: catboost\n",
      "0:\tlearn: 1.2636855\ttotal: 245ms\tremaining: 8m 10s\n",
      "50:\tlearn: 0.9845347\ttotal: 9.27s\tremaining: 5m 54s\n",
      "100:\tlearn: 0.9467891\ttotal: 18.9s\tremaining: 5m 54s\n",
      "150:\tlearn: 0.9350999\ttotal: 28.6s\tremaining: 5m 50s\n",
      "200:\tlearn: 0.9289819\ttotal: 38.6s\tremaining: 5m 45s\n",
      "250:\tlearn: 0.9248226\ttotal: 48.7s\tremaining: 5m 39s\n",
      "300:\tlearn: 0.9193514\ttotal: 59s\tremaining: 5m 33s\n",
      "350:\tlearn: 0.9152618\ttotal: 1m 9s\tremaining: 5m 26s\n",
      "400:\tlearn: 0.9125111\ttotal: 1m 19s\tremaining: 5m 18s\n",
      "450:\tlearn: 0.9090242\ttotal: 1m 30s\tremaining: 5m 9s\n",
      "500:\tlearn: 0.9064601\ttotal: 1m 40s\tremaining: 5m\n",
      "550:\tlearn: 0.9043502\ttotal: 1m 50s\tremaining: 4m 50s\n",
      "600:\tlearn: 0.9022138\ttotal: 2m\tremaining: 4m 40s\n",
      "650:\tlearn: 0.9006280\ttotal: 2m 10s\tremaining: 4m 30s\n",
      "700:\tlearn: 0.8988053\ttotal: 2m 20s\tremaining: 4m 20s\n",
      "750:\tlearn: 0.8973365\ttotal: 2m 30s\tremaining: 4m 10s\n",
      "800:\tlearn: 0.8957243\ttotal: 2m 40s\tremaining: 4m\n",
      "850:\tlearn: 0.8944413\ttotal: 2m 50s\tremaining: 3m 50s\n",
      "900:\tlearn: 0.8934180\ttotal: 3m\tremaining: 3m 40s\n",
      "950:\tlearn: 0.8917539\ttotal: 3m 10s\tremaining: 3m 30s\n",
      "1000:\tlearn: 0.8904386\ttotal: 3m 20s\tremaining: 3m 20s\n",
      "1050:\tlearn: 0.8893589\ttotal: 3m 30s\tremaining: 3m 10s\n",
      "1100:\tlearn: 0.8879622\ttotal: 3m 41s\tremaining: 3m\n",
      "1150:\tlearn: 0.8871306\ttotal: 3m 51s\tremaining: 2m 50s\n",
      "1200:\tlearn: 0.8862052\ttotal: 4m\tremaining: 2m 40s\n",
      "1250:\tlearn: 0.8851276\ttotal: 4m 10s\tremaining: 2m 30s\n",
      "1300:\tlearn: 0.8840216\ttotal: 4m 20s\tremaining: 2m 20s\n",
      "1350:\tlearn: 0.8831140\ttotal: 4m 30s\tremaining: 2m 10s\n",
      "1400:\tlearn: 0.8821712\ttotal: 4m 40s\tremaining: 2m\n",
      "1450:\tlearn: 0.8813976\ttotal: 4m 50s\tremaining: 1m 50s\n",
      "1500:\tlearn: 0.8807112\ttotal: 5m\tremaining: 1m 40s\n",
      "1550:\tlearn: 0.8800018\ttotal: 5m 10s\tremaining: 1m 30s\n",
      "1600:\tlearn: 0.8792006\ttotal: 5m 20s\tremaining: 1m 19s\n",
      "1650:\tlearn: 0.8783719\ttotal: 5m 30s\tremaining: 1m 9s\n",
      "1700:\tlearn: 0.8776777\ttotal: 5m 40s\tremaining: 59.9s\n",
      "1750:\tlearn: 0.8771189\ttotal: 5m 50s\tremaining: 49.9s\n",
      "1800:\tlearn: 0.8764647\ttotal: 6m 1s\tremaining: 39.9s\n",
      "1850:\tlearn: 0.8757465\ttotal: 6m 11s\tremaining: 29.9s\n",
      "1900:\tlearn: 0.8750958\ttotal: 6m 21s\tremaining: 19.9s\n",
      "1950:\tlearn: 0.8744972\ttotal: 6m 31s\tremaining: 9.84s\n",
      "1999:\tlearn: 0.8739841\ttotal: 6m 41s\tremaining: 0us\n",
      "catboost runs for 414.60 seconds.\n",
      "Training Model 4: random forest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▎        | 1/8 [15:05<1:45:35, 905.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest runs for 346.99 seconds.\n",
      "Total running time was 15.08 minutes.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Start training for month28\n",
      "Training Model 0: sgdr\n",
      "SGDRegressor runs for 5.11 seconds.\n",
      "\n",
      "Training Model 1: lightgbm\n",
      "lightgbm runs for 131.56 seconds.\n",
      "\n",
      "Training Model 2: keras\n",
      "Epoch 1/5\n",
      "3968296/3968296 [==============================] - 2s 1us/step - loss: 9.4576 - mean_squared_error: 9.4576\n",
      "Epoch 2/5\n",
      "3968296/3968296 [==============================] - 2s 1us/step - loss: 1.6182 - mean_squared_error: 1.6182\n",
      "Epoch 3/5\n",
      "3968296/3968296 [==============================] - 2s 1us/step - loss: 1.6182 - mean_squared_error: 1.6182\n",
      "Epoch 4/5\n",
      "3968296/3968296 [==============================] - 2s 1us/step - loss: 1.6182 - mean_squared_error: 1.6182\n",
      "Epoch 5/5\n",
      "3968296/3968296 [==============================] - 2s 1us/step - loss: 1.6182 - mean_squared_error: 1.6182\n",
      "212503/212503 [==============================] - 0s 0us/step\n",
      "keras runs for 11.79 seconds.\n",
      "Total running time was 2.50 minutes.\n",
      "Training Model 3: catboost\n",
      "0:\tlearn: 1.2574700\ttotal: 196ms\tremaining: 6m 31s\n",
      "50:\tlearn: 0.9834394\ttotal: 9.73s\tremaining: 6m 11s\n",
      "100:\tlearn: 0.9464049\ttotal: 19.7s\tremaining: 6m 11s\n",
      "150:\tlearn: 0.9353168\ttotal: 30.1s\tremaining: 6m 8s\n",
      "200:\tlearn: 0.9290600\ttotal: 41s\tremaining: 6m 6s\n",
      "250:\tlearn: 0.9238613\ttotal: 52.1s\tremaining: 6m 3s\n",
      "300:\tlearn: 0.9193515\ttotal: 1m 3s\tremaining: 5m 58s\n",
      "350:\tlearn: 0.9153783\ttotal: 1m 14s\tremaining: 5m 52s\n",
      "400:\tlearn: 0.9119612\ttotal: 1m 25s\tremaining: 5m 42s\n",
      "450:\tlearn: 0.9095185\ttotal: 1m 36s\tremaining: 5m 32s\n",
      "500:\tlearn: 0.9069118\ttotal: 1m 47s\tremaining: 5m 22s\n",
      "550:\tlearn: 0.9051469\ttotal: 1m 58s\tremaining: 5m 11s\n",
      "600:\tlearn: 0.9028466\ttotal: 2m 9s\tremaining: 5m 1s\n",
      "650:\tlearn: 0.9009071\ttotal: 2m 20s\tremaining: 4m 51s\n",
      "700:\tlearn: 0.8992833\ttotal: 2m 31s\tremaining: 4m 40s\n",
      "750:\tlearn: 0.8979255\ttotal: 2m 42s\tremaining: 4m 30s\n",
      "800:\tlearn: 0.8962714\ttotal: 2m 53s\tremaining: 4m 19s\n",
      "850:\tlearn: 0.8947200\ttotal: 3m 4s\tremaining: 4m 8s\n",
      "900:\tlearn: 0.8936594\ttotal: 3m 14s\tremaining: 3m 57s\n",
      "950:\tlearn: 0.8921142\ttotal: 3m 25s\tremaining: 3m 46s\n",
      "1000:\tlearn: 0.8907203\ttotal: 3m 36s\tremaining: 3m 36s\n",
      "1050:\tlearn: 0.8893951\ttotal: 3m 47s\tremaining: 3m 25s\n",
      "1100:\tlearn: 0.8884156\ttotal: 3m 58s\tremaining: 3m 14s\n",
      "1150:\tlearn: 0.8873816\ttotal: 4m 8s\tremaining: 3m 3s\n",
      "1200:\tlearn: 0.8864797\ttotal: 4m 19s\tremaining: 2m 52s\n",
      "1250:\tlearn: 0.8857397\ttotal: 4m 30s\tremaining: 2m 41s\n",
      "1300:\tlearn: 0.8846320\ttotal: 4m 40s\tremaining: 2m 30s\n",
      "1350:\tlearn: 0.8837455\ttotal: 4m 51s\tremaining: 2m 20s\n",
      "1400:\tlearn: 0.8830451\ttotal: 5m 2s\tremaining: 2m 9s\n",
      "1450:\tlearn: 0.8823456\ttotal: 5m 13s\tremaining: 1m 58s\n",
      "1500:\tlearn: 0.8815678\ttotal: 5m 23s\tremaining: 1m 47s\n",
      "1550:\tlearn: 0.8808380\ttotal: 5m 35s\tremaining: 1m 37s\n",
      "1600:\tlearn: 0.8802359\ttotal: 5m 47s\tremaining: 1m 26s\n",
      "1650:\tlearn: 0.8794509\ttotal: 5m 58s\tremaining: 1m 15s\n",
      "1700:\tlearn: 0.8786560\ttotal: 6m 10s\tremaining: 1m 5s\n",
      "1750:\tlearn: 0.8779908\ttotal: 6m 21s\tremaining: 54.2s\n",
      "1800:\tlearn: 0.8772941\ttotal: 6m 32s\tremaining: 43.3s\n",
      "1850:\tlearn: 0.8766857\ttotal: 6m 42s\tremaining: 32.4s\n",
      "1900:\tlearn: 0.8759863\ttotal: 6m 53s\tremaining: 21.5s\n",
      "1950:\tlearn: 0.8753238\ttotal: 7m 4s\tremaining: 10.7s\n",
      "1999:\tlearn: 0.8747823\ttotal: 7m 15s\tremaining: 0us\n",
      "catboost runs for 449.13 seconds.\n",
      "Training Model 4: random forest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 2/8 [31:58<1:33:45, 937.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest runs for 414.38 seconds.\n",
      "Total running time was 16.89 minutes.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Start training for month29\n",
      "Training Model 0: sgdr\n",
      "SGDRegressor runs for 5.46 seconds.\n",
      "\n",
      "Training Model 1: lightgbm\n",
      "lightgbm runs for 145.55 seconds.\n",
      "\n",
      "Training Model 2: keras\n",
      "Epoch 1/5\n",
      "4180799/4180799 [==============================] - 3s 1us/step - loss: 52.7795 - mean_squared_error: 52.7795\n",
      "Epoch 2/5\n",
      "4180799/4180799 [==============================] - 2s 1us/step - loss: 1.5991 - mean_squared_error: 1.5991\n",
      "Epoch 3/5\n",
      "4180799/4180799 [==============================] - 2s 1us/step - loss: 1.5991 - mean_squared_error: 1.5991\n",
      "Epoch 4/5\n",
      "4180799/4180799 [==============================] - 2s 1us/step - loss: 1.5991 - mean_squared_error: 1.5991\n",
      "Epoch 5/5\n",
      "4180799/4180799 [==============================] - 2s 1us/step - loss: 1.5991 - mean_squared_error: 1.5991\n",
      "210494/210494 [==============================] - 0s 0us/step\n",
      "keras runs for 12.49 seconds.\n",
      "Total running time was 2.75 minutes.\n",
      "Training Model 3: catboost\n",
      "0:\tlearn: 1.2501455\ttotal: 256ms\tremaining: 8m 31s\n",
      "50:\tlearn: 0.9801081\ttotal: 14.4s\tremaining: 9m 9s\n",
      "100:\tlearn: 0.9435125\ttotal: 28.4s\tremaining: 8m 54s\n",
      "150:\tlearn: 0.9323570\ttotal: 42.5s\tremaining: 8m 40s\n",
      "200:\tlearn: 0.9263091\ttotal: 56.6s\tremaining: 8m 26s\n",
      "250:\tlearn: 0.9211305\ttotal: 1m 10s\tremaining: 8m 13s\n",
      "300:\tlearn: 0.9170376\ttotal: 1m 24s\tremaining: 7m 57s\n",
      "350:\tlearn: 0.9120739\ttotal: 1m 38s\tremaining: 7m 44s\n",
      "400:\tlearn: 0.9086450\ttotal: 1m 53s\tremaining: 7m 32s\n",
      "450:\tlearn: 0.9060953\ttotal: 2m 8s\tremaining: 7m 20s\n",
      "500:\tlearn: 0.9032892\ttotal: 2m 22s\tremaining: 7m 6s\n",
      "550:\tlearn: 0.9008409\ttotal: 2m 38s\tremaining: 6m 56s\n",
      "600:\tlearn: 0.8988057\ttotal: 2m 53s\tremaining: 6m 44s\n",
      "650:\tlearn: 0.8971489\ttotal: 3m 10s\tremaining: 6m 34s\n",
      "700:\tlearn: 0.8956562\ttotal: 3m 24s\tremaining: 6m 19s\n",
      "750:\tlearn: 0.8941355\ttotal: 3m 38s\tremaining: 6m 3s\n",
      "800:\tlearn: 0.8927412\ttotal: 3m 52s\tremaining: 5m 48s\n",
      "850:\tlearn: 0.8912864\ttotal: 4m 7s\tremaining: 5m 34s\n",
      "900:\tlearn: 0.8902379\ttotal: 4m 23s\tremaining: 5m 20s\n",
      "950:\tlearn: 0.8890895\ttotal: 4m 38s\tremaining: 5m 6s\n",
      "1000:\tlearn: 0.8879906\ttotal: 4m 54s\tremaining: 4m 53s\n",
      "1050:\tlearn: 0.8867917\ttotal: 5m 9s\tremaining: 4m 39s\n",
      "1100:\tlearn: 0.8856549\ttotal: 5m 24s\tremaining: 4m 24s\n",
      "1150:\tlearn: 0.8847750\ttotal: 5m 39s\tremaining: 4m 10s\n",
      "1200:\tlearn: 0.8835308\ttotal: 5m 54s\tremaining: 3m 55s\n",
      "1250:\tlearn: 0.8825354\ttotal: 6m 9s\tremaining: 3m 41s\n",
      "1300:\tlearn: 0.8812566\ttotal: 6m 24s\tremaining: 3m 26s\n",
      "1350:\tlearn: 0.8804169\ttotal: 6m 39s\tremaining: 3m 11s\n",
      "1400:\tlearn: 0.8793802\ttotal: 6m 54s\tremaining: 2m 57s\n",
      "1450:\tlearn: 0.8785371\ttotal: 7m 10s\tremaining: 2m 42s\n",
      "1500:\tlearn: 0.8776620\ttotal: 7m 25s\tremaining: 2m 28s\n",
      "1550:\tlearn: 0.8769493\ttotal: 7m 40s\tremaining: 2m 13s\n",
      "1600:\tlearn: 0.8762238\ttotal: 7m 56s\tremaining: 1m 58s\n",
      "1650:\tlearn: 0.8756471\ttotal: 8m 11s\tremaining: 1m 43s\n",
      "1700:\tlearn: 0.8750875\ttotal: 8m 26s\tremaining: 1m 29s\n",
      "1750:\tlearn: 0.8744201\ttotal: 8m 42s\tremaining: 1m 14s\n",
      "1800:\tlearn: 0.8738298\ttotal: 8m 58s\tremaining: 59.5s\n",
      "1850:\tlearn: 0.8731097\ttotal: 9m 14s\tremaining: 44.6s\n",
      "1900:\tlearn: 0.8725140\ttotal: 9m 29s\tremaining: 29.7s\n",
      "1950:\tlearn: 0.8719229\ttotal: 9m 44s\tremaining: 14.7s\n",
      "1999:\tlearn: 0.8713805\ttotal: 9m 59s\tremaining: 0us\n",
      "catboost runs for 615.34 seconds.\n",
      "Training Model 4: random forest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 3/8 [52:16<1:25:07, 1021.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest runs for 437.42 seconds.\n",
      "Total running time was 20.30 minutes.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Start training for month30\n",
      "Training Model 0: sgdr\n",
      "SGDRegressor runs for 6.83 seconds.\n",
      "\n",
      "Training Model 1: lightgbm\n",
      "lightgbm runs for 163.01 seconds.\n",
      "\n",
      "Training Model 2: keras\n",
      "Epoch 1/5\n",
      "4391293/4391293 [==============================] - 3s 1us/step - loss: 1.7163 - mean_squared_error: 1.7163\n",
      "Epoch 2/5\n",
      "4391293/4391293 [==============================] - 3s 1us/step - loss: 1.5792 - mean_squared_error: 1.5792: 0s - loss: 1.5796 - mean_squared_error: 1.57\n",
      "Epoch 3/5\n",
      "4391293/4391293 [==============================] - 3s 1us/step - loss: 1.5792 - mean_squared_error: 1.5792\n",
      "Epoch 4/5\n",
      "4391293/4391293 [==============================] - 3s 1us/step - loss: 1.5792 - mean_squared_error: 1.5792\n",
      "Epoch 5/5\n",
      "4391293/4391293 [==============================] - 3s 1us/step - loss: 1.5792 - mean_squared_error: 1.5792\n",
      "215496/215496 [==============================] - 0s 0us/step\n",
      "keras runs for 14.11 seconds.\n",
      "Total running time was 3.09 minutes.\n",
      "Training Model 3: catboost\n",
      "0:\tlearn: 1.2423586\ttotal: 269ms\tremaining: 8m 57s\n",
      "50:\tlearn: 0.9741294\ttotal: 13s\tremaining: 8m 16s\n",
      "100:\tlearn: 0.9383155\ttotal: 26.1s\tremaining: 8m 11s\n",
      "150:\tlearn: 0.9273342\ttotal: 39.8s\tremaining: 8m 6s\n",
      "200:\tlearn: 0.9213312\ttotal: 54.2s\tremaining: 8m 5s\n",
      "250:\tlearn: 0.9169896\ttotal: 1m 8s\tremaining: 7m 56s\n",
      "300:\tlearn: 0.9115606\ttotal: 1m 22s\tremaining: 7m 46s\n",
      "350:\tlearn: 0.9073029\ttotal: 1m 37s\tremaining: 7m 37s\n",
      "400:\tlearn: 0.9034487\ttotal: 1m 52s\tremaining: 7m 26s\n",
      "450:\tlearn: 0.9005806\ttotal: 2m 6s\tremaining: 7m 13s\n",
      "500:\tlearn: 0.8982658\ttotal: 2m 20s\tremaining: 7m\n",
      "550:\tlearn: 0.8963619\ttotal: 2m 34s\tremaining: 6m 45s\n",
      "600:\tlearn: 0.8948960\ttotal: 2m 48s\tremaining: 6m 31s\n",
      "650:\tlearn: 0.8932455\ttotal: 3m 2s\tremaining: 6m 17s\n",
      "700:\tlearn: 0.8916627\ttotal: 3m 16s\tremaining: 6m 3s\n",
      "750:\tlearn: 0.8901283\ttotal: 3m 30s\tremaining: 5m 49s\n",
      "800:\tlearn: 0.8886902\ttotal: 3m 44s\tremaining: 5m 35s\n",
      "850:\tlearn: 0.8869708\ttotal: 3m 58s\tremaining: 5m 22s\n",
      "900:\tlearn: 0.8852725\ttotal: 4m 13s\tremaining: 5m 8s\n",
      "950:\tlearn: 0.8839082\ttotal: 4m 27s\tremaining: 4m 54s\n",
      "1000:\tlearn: 0.8828232\ttotal: 4m 41s\tremaining: 4m 40s\n",
      "1050:\tlearn: 0.8819465\ttotal: 4m 55s\tremaining: 4m 26s\n",
      "1100:\tlearn: 0.8806572\ttotal: 5m 9s\tremaining: 4m 12s\n",
      "1150:\tlearn: 0.8797710\ttotal: 5m 23s\tremaining: 3m 58s\n",
      "1200:\tlearn: 0.8788112\ttotal: 5m 37s\tremaining: 3m 44s\n",
      "1250:\tlearn: 0.8778221\ttotal: 5m 50s\tremaining: 3m 30s\n",
      "1300:\tlearn: 0.8769525\ttotal: 6m 5s\tremaining: 3m 16s\n",
      "1350:\tlearn: 0.8760763\ttotal: 6m 19s\tremaining: 3m 2s\n",
      "1400:\tlearn: 0.8751154\ttotal: 6m 33s\tremaining: 2m 48s\n",
      "1450:\tlearn: 0.8743602\ttotal: 6m 48s\tremaining: 2m 34s\n",
      "1500:\tlearn: 0.8736593\ttotal: 7m 1s\tremaining: 2m 20s\n",
      "1550:\tlearn: 0.8730013\ttotal: 7m 14s\tremaining: 2m 5s\n",
      "1600:\tlearn: 0.8723243\ttotal: 7m 28s\tremaining: 1m 51s\n",
      "1650:\tlearn: 0.8715644\ttotal: 7m 42s\tremaining: 1m 37s\n",
      "1700:\tlearn: 0.8708911\ttotal: 7m 57s\tremaining: 1m 23s\n",
      "1750:\tlearn: 0.8701869\ttotal: 8m 11s\tremaining: 1m 9s\n",
      "1800:\tlearn: 0.8695312\ttotal: 8m 24s\tremaining: 55.8s\n",
      "1850:\tlearn: 0.8689058\ttotal: 8m 38s\tremaining: 41.8s\n",
      "1900:\tlearn: 0.8683473\ttotal: 8m 52s\tremaining: 27.7s\n",
      "1950:\tlearn: 0.8678010\ttotal: 9m 6s\tremaining: 13.7s\n",
      "1999:\tlearn: 0.8672992\ttotal: 9m 20s\tremaining: 0us\n",
      "catboost runs for 577.00 seconds.\n",
      "Training Model 4: random forest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 4/8 [1:12:43<1:12:13, 1083.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest runs for 464.71 seconds.\n",
      "Total running time was 20.45 minutes.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Start training for month31\n",
      "Training Model 0: sgdr\n",
      "SGDRegressor runs for 7.18 seconds.\n",
      "\n",
      "Training Model 1: lightgbm\n",
      "lightgbm runs for 156.34 seconds.\n",
      "\n",
      "Training Model 2: keras\n",
      "Epoch 1/5\n",
      "4606789/4606789 [==============================] - 3s 1us/step - loss: 1.5548 - mean_squared_error: 1.5548\n",
      "Epoch 2/5\n",
      "4606789/4606789 [==============================] - 3s 1us/step - loss: 1.5548 - mean_squared_error: 1.5548\n",
      "Epoch 3/5\n",
      "4606789/4606789 [==============================] - 3s 1us/step - loss: 1.5548 - mean_squared_error: 1.5548\n",
      "Epoch 4/5\n",
      "4606789/4606789 [==============================] - 3s 1us/step - loss: 1.5548 - mean_squared_error: 1.5548\n",
      "Epoch 5/5\n",
      "4606789/4606789 [==============================] - 3s 1us/step - loss: 1.5548 - mean_squared_error: 1.5548\n",
      "208444/208444 [==============================] - 0s 0us/step\n",
      "keras runs for 13.57 seconds.\n",
      "Total running time was 2.98 minutes.\n",
      "Training Model 3: catboost\n",
      "0:\tlearn: 1.2326826\ttotal: 255ms\tremaining: 8m 29s\n",
      "50:\tlearn: 0.9654811\ttotal: 12.9s\tremaining: 8m 13s\n",
      "100:\tlearn: 0.9298075\ttotal: 25.9s\tremaining: 8m 6s\n",
      "150:\tlearn: 0.9185771\ttotal: 39.1s\tremaining: 7m 58s\n",
      "200:\tlearn: 0.9130789\ttotal: 52.6s\tremaining: 7m 50s\n",
      "250:\tlearn: 0.9089173\ttotal: 1m 6s\tremaining: 7m 42s\n",
      "300:\tlearn: 0.9046505\ttotal: 1m 20s\tremaining: 7m 32s\n",
      "350:\tlearn: 0.8997569\ttotal: 1m 34s\tremaining: 7m 22s\n",
      "400:\tlearn: 0.8967851\ttotal: 1m 47s\tremaining: 7m 8s\n",
      "450:\tlearn: 0.8944587\ttotal: 2m 1s\tremaining: 6m 57s\n",
      "500:\tlearn: 0.8918200\ttotal: 2m 15s\tremaining: 6m 44s\n",
      "550:\tlearn: 0.8895305\ttotal: 2m 28s\tremaining: 6m 29s\n",
      "600:\tlearn: 0.8878797\ttotal: 2m 41s\tremaining: 6m 16s\n",
      "650:\tlearn: 0.8865235\ttotal: 2m 55s\tremaining: 6m 3s\n",
      "700:\tlearn: 0.8845728\ttotal: 3m 8s\tremaining: 5m 49s\n",
      "750:\tlearn: 0.8832135\ttotal: 3m 22s\tremaining: 5m 36s\n",
      "800:\tlearn: 0.8818669\ttotal: 3m 36s\tremaining: 5m 24s\n",
      "850:\tlearn: 0.8807352\ttotal: 3m 50s\tremaining: 5m 10s\n",
      "900:\tlearn: 0.8794564\ttotal: 4m 3s\tremaining: 4m 56s\n",
      "950:\tlearn: 0.8780094\ttotal: 4m 16s\tremaining: 4m 42s\n",
      "1000:\tlearn: 0.8768036\ttotal: 4m 29s\tremaining: 4m 29s\n",
      "1050:\tlearn: 0.8753133\ttotal: 4m 42s\tremaining: 4m 15s\n",
      "1100:\tlearn: 0.8738383\ttotal: 4m 55s\tremaining: 4m 1s\n",
      "1150:\tlearn: 0.8728014\ttotal: 5m 9s\tremaining: 3m 48s\n",
      "1200:\tlearn: 0.8717872\ttotal: 5m 22s\tremaining: 3m 34s\n",
      "1250:\tlearn: 0.8708887\ttotal: 5m 35s\tremaining: 3m 20s\n",
      "1300:\tlearn: 0.8701055\ttotal: 5m 48s\tremaining: 3m 7s\n",
      "1350:\tlearn: 0.8693383\ttotal: 6m 1s\tremaining: 2m 53s\n",
      "1400:\tlearn: 0.8684976\ttotal: 6m 15s\tremaining: 2m 40s\n",
      "1450:\tlearn: 0.8678427\ttotal: 6m 30s\tremaining: 2m 27s\n",
      "1500:\tlearn: 0.8670502\ttotal: 6m 45s\tremaining: 2m 14s\n",
      "1550:\tlearn: 0.8663918\ttotal: 6m 59s\tremaining: 2m 1s\n",
      "1600:\tlearn: 0.8657908\ttotal: 7m 13s\tremaining: 1m 47s\n",
      "1650:\tlearn: 0.8651195\ttotal: 7m 27s\tremaining: 1m 34s\n",
      "1700:\tlearn: 0.8644439\ttotal: 7m 41s\tremaining: 1m 21s\n",
      "1750:\tlearn: 0.8638926\ttotal: 7m 55s\tremaining: 1m 7s\n",
      "1800:\tlearn: 0.8634084\ttotal: 8m 9s\tremaining: 54s\n",
      "1850:\tlearn: 0.8628456\ttotal: 8m 22s\tremaining: 40.5s\n",
      "1900:\tlearn: 0.8623319\ttotal: 8m 36s\tremaining: 26.9s\n",
      "1950:\tlearn: 0.8618709\ttotal: 8m 50s\tremaining: 13.3s\n",
      "1999:\tlearn: 0.8614051\ttotal: 9m 3s\tremaining: 0us\n",
      "catboost runs for 560.70 seconds.\n",
      "Training Model 4: random forest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████▎   | 5/8 [1:32:43<55:55, 1118.46s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest runs for 460.95 seconds.\n",
      "Total running time was 20.01 minutes.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Start training for month32\n",
      "Training Model 0: sgdr\n",
      "SGDRegressor runs for 6.96 seconds.\n",
      "\n",
      "Training Model 1: lightgbm\n",
      "lightgbm runs for 165.39 seconds.\n",
      "\n",
      "Training Model 2: keras\n",
      "Epoch 1/5\n",
      "4815233/4815233 [==============================] - 3s 1us/step - loss: 154.2046 - mean_squared_error: 154.2046\n",
      "Epoch 2/5\n",
      "4815233/4815233 [==============================] - 3s 1us/step - loss: 1.5412 - mean_squared_error: 1.5412\n",
      "Epoch 3/5\n",
      "4815233/4815233 [==============================] - 3s 1us/step - loss: 1.5412 - mean_squared_error: 1.5412\n",
      "Epoch 4/5\n",
      "4815233/4815233 [==============================] - 3s 1us/step - loss: 1.5412 - mean_squared_error: 1.5412\n",
      "Epoch 5/5\n",
      "4815233/4815233 [==============================] - 3s 1us/step - loss: 1.5412 - mean_squared_error: 1.5412\n",
      "208075/208075 [==============================] - 0s 0us/step\n",
      "keras runs for 14.21 seconds.\n",
      "Total running time was 3.14 minutes.\n",
      "Training Model 3: catboost\n",
      "0:\tlearn: 1.2272734\ttotal: 280ms\tremaining: 9m 19s\n",
      "50:\tlearn: 0.9614218\ttotal: 13.1s\tremaining: 8m 18s\n",
      "100:\tlearn: 0.9259372\ttotal: 26.6s\tremaining: 8m 19s\n",
      "150:\tlearn: 0.9148786\ttotal: 40.4s\tremaining: 8m 15s\n",
      "200:\tlearn: 0.9092729\ttotal: 54.8s\tremaining: 8m 10s\n",
      "250:\tlearn: 0.9050834\ttotal: 1m 10s\tremaining: 8m 12s\n",
      "300:\tlearn: 0.9015660\ttotal: 1m 26s\tremaining: 8m 6s\n",
      "350:\tlearn: 0.8974736\ttotal: 1m 41s\tremaining: 7m 58s\n",
      "400:\tlearn: 0.8936530\ttotal: 1m 56s\tremaining: 7m 44s\n",
      "450:\tlearn: 0.8906903\ttotal: 2m 11s\tremaining: 7m 30s\n",
      "500:\tlearn: 0.8884302\ttotal: 2m 25s\tremaining: 7m 16s\n",
      "550:\tlearn: 0.8862641\ttotal: 2m 40s\tremaining: 7m 1s\n",
      "600:\tlearn: 0.8846597\ttotal: 2m 54s\tremaining: 6m 47s\n",
      "650:\tlearn: 0.8829114\ttotal: 3m 10s\tremaining: 6m 33s\n",
      "700:\tlearn: 0.8813981\ttotal: 3m 24s\tremaining: 6m 19s\n",
      "750:\tlearn: 0.8802755\ttotal: 3m 39s\tremaining: 6m 4s\n",
      "800:\tlearn: 0.8788283\ttotal: 3m 54s\tremaining: 5m 51s\n",
      "850:\tlearn: 0.8775557\ttotal: 4m 9s\tremaining: 5m 37s\n",
      "900:\tlearn: 0.8760925\ttotal: 4m 24s\tremaining: 5m 22s\n",
      "950:\tlearn: 0.8751323\ttotal: 4m 39s\tremaining: 5m 8s\n",
      "1000:\tlearn: 0.8737881\ttotal: 4m 54s\tremaining: 4m 53s\n",
      "1050:\tlearn: 0.8727363\ttotal: 5m 9s\tremaining: 4m 39s\n",
      "1100:\tlearn: 0.8716312\ttotal: 5m 23s\tremaining: 4m 24s\n",
      "1150:\tlearn: 0.8705883\ttotal: 5m 37s\tremaining: 4m 8s\n",
      "1200:\tlearn: 0.8694785\ttotal: 5m 51s\tremaining: 3m 54s\n",
      "1250:\tlearn: 0.8681828\ttotal: 6m 5s\tremaining: 3m 39s\n",
      "1300:\tlearn: 0.8673039\ttotal: 6m 20s\tremaining: 3m 24s\n",
      "1350:\tlearn: 0.8665659\ttotal: 6m 34s\tremaining: 3m 9s\n",
      "1400:\tlearn: 0.8657444\ttotal: 6m 49s\tremaining: 2m 55s\n",
      "1450:\tlearn: 0.8650041\ttotal: 7m 4s\tremaining: 2m 40s\n",
      "1500:\tlearn: 0.8642441\ttotal: 7m 18s\tremaining: 2m 25s\n",
      "1550:\tlearn: 0.8635800\ttotal: 7m 32s\tremaining: 2m 10s\n",
      "1600:\tlearn: 0.8630097\ttotal: 7m 46s\tremaining: 1m 56s\n",
      "1650:\tlearn: 0.8623598\ttotal: 8m\tremaining: 1m 41s\n",
      "1700:\tlearn: 0.8616442\ttotal: 8m 14s\tremaining: 1m 26s\n",
      "1750:\tlearn: 0.8609333\ttotal: 8m 28s\tremaining: 1m 12s\n",
      "1800:\tlearn: 0.8603087\ttotal: 8m 42s\tremaining: 57.7s\n",
      "1850:\tlearn: 0.8597514\ttotal: 8m 56s\tremaining: 43.2s\n",
      "1900:\tlearn: 0.8591946\ttotal: 9m 9s\tremaining: 28.6s\n",
      "1950:\tlearn: 0.8586853\ttotal: 9m 23s\tremaining: 14.2s\n",
      "1999:\tlearn: 0.8581618\ttotal: 9m 36s\tremaining: 0us\n",
      "catboost runs for 593.94 seconds.\n",
      "Training Model 4: random forest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 6/8 [1:53:39<38:39, 1159.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest runs for 473.12 seconds.\n",
      "Total running time was 20.92 minutes.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Start training for month33\n",
      "Training Model 0: sgdr\n",
      "SGDRegressor runs for 6.81 seconds.\n",
      "\n",
      "Training Model 1: lightgbm\n",
      "lightgbm runs for 173.46 seconds.\n",
      "\n",
      "Training Model 2: keras\n",
      "Epoch 1/5\n",
      "5023308/5023308 [==============================] - 3s 1us/step - loss: 1.5806 - mean_squared_error: 1.5806\n",
      "Epoch 2/5\n",
      "5023308/5023308 [==============================] - 3s 1us/step - loss: 1.5382 - mean_squared_error: 1.5382\n",
      "Epoch 3/5\n",
      "5023308/5023308 [==============================] - 3s 1us/step - loss: 1.5382 - mean_squared_error: 1.5382\n",
      "Epoch 4/5\n",
      "5023308/5023308 [==============================] - 3s 1us/step - loss: 1.5382 - mean_squared_error: 1.5382\n",
      "Epoch 5/5\n",
      "5023308/5023308 [==============================] - 3s 1us/step - loss: 1.5382 - mean_squared_error: 1.5382\n",
      "221802/221802 [==============================] - 0s 0us/step\n",
      "keras runs for 15.54 seconds.\n",
      "Total running time was 3.29 minutes.\n",
      "Training Model 3: catboost\n",
      "0:\tlearn: 1.2261800\ttotal: 266ms\tremaining: 8m 52s\n",
      "50:\tlearn: 0.9626543\ttotal: 14.2s\tremaining: 9m 2s\n",
      "100:\tlearn: 0.9277334\ttotal: 29.1s\tremaining: 9m 7s\n",
      "150:\tlearn: 0.9171047\ttotal: 45.2s\tremaining: 9m 13s\n",
      "200:\tlearn: 0.9113385\ttotal: 1m 1s\tremaining: 9m 8s\n",
      "250:\tlearn: 0.9062300\ttotal: 1m 17s\tremaining: 8m 57s\n",
      "300:\tlearn: 0.9022438\ttotal: 1m 32s\tremaining: 8m 43s\n",
      "350:\tlearn: 0.8979668\ttotal: 1m 47s\tremaining: 8m 27s\n",
      "400:\tlearn: 0.8947017\ttotal: 2m 3s\tremaining: 8m 11s\n",
      "450:\tlearn: 0.8916455\ttotal: 2m 18s\tremaining: 7m 55s\n",
      "500:\tlearn: 0.8894801\ttotal: 2m 33s\tremaining: 7m 39s\n",
      "550:\tlearn: 0.8875657\ttotal: 2m 48s\tremaining: 7m 23s\n",
      "600:\tlearn: 0.8857294\ttotal: 3m 3s\tremaining: 7m 6s\n",
      "650:\tlearn: 0.8839034\ttotal: 3m 18s\tremaining: 6m 50s\n",
      "700:\tlearn: 0.8823427\ttotal: 3m 33s\tremaining: 6m 34s\n",
      "750:\tlearn: 0.8811902\ttotal: 3m 47s\tremaining: 6m 18s\n",
      "800:\tlearn: 0.8796374\ttotal: 4m 2s\tremaining: 6m 2s\n",
      "850:\tlearn: 0.8780416\ttotal: 4m 16s\tremaining: 5m 46s\n",
      "900:\tlearn: 0.8764683\ttotal: 4m 31s\tremaining: 5m 31s\n",
      "950:\tlearn: 0.8752962\ttotal: 4m 46s\tremaining: 5m 15s\n",
      "1000:\tlearn: 0.8742340\ttotal: 5m\tremaining: 5m\n",
      "1050:\tlearn: 0.8733358\ttotal: 5m 15s\tremaining: 4m 44s\n",
      "1100:\tlearn: 0.8722407\ttotal: 5m 29s\tremaining: 4m 29s\n",
      "1150:\tlearn: 0.8712105\ttotal: 5m 43s\tremaining: 4m 13s\n",
      "1200:\tlearn: 0.8704158\ttotal: 5m 58s\tremaining: 3m 58s\n",
      "1250:\tlearn: 0.8694483\ttotal: 6m 12s\tremaining: 3m 43s\n",
      "1300:\tlearn: 0.8686361\ttotal: 6m 27s\tremaining: 3m 28s\n",
      "1350:\tlearn: 0.8678666\ttotal: 6m 41s\tremaining: 3m 12s\n",
      "1400:\tlearn: 0.8670531\ttotal: 6m 55s\tremaining: 2m 57s\n",
      "1450:\tlearn: 0.8664144\ttotal: 7m 10s\tremaining: 2m 42s\n",
      "1500:\tlearn: 0.8655612\ttotal: 7m 25s\tremaining: 2m 27s\n",
      "1550:\tlearn: 0.8649219\ttotal: 7m 40s\tremaining: 2m 13s\n",
      "1600:\tlearn: 0.8642789\ttotal: 7m 55s\tremaining: 1m 58s\n",
      "1650:\tlearn: 0.8635191\ttotal: 8m 10s\tremaining: 1m 43s\n",
      "1700:\tlearn: 0.8629585\ttotal: 8m 24s\tremaining: 1m 28s\n",
      "1750:\tlearn: 0.8624577\ttotal: 8m 40s\tremaining: 1m 13s\n",
      "1800:\tlearn: 0.8618753\ttotal: 8m 55s\tremaining: 59.1s\n",
      "1850:\tlearn: 0.8613072\ttotal: 9m 9s\tremaining: 44.2s\n",
      "1900:\tlearn: 0.8607769\ttotal: 9m 23s\tremaining: 29.4s\n",
      "1950:\tlearn: 0.8603057\ttotal: 9m 38s\tremaining: 14.5s\n",
      "1999:\tlearn: 0.8597731\ttotal: 9m 52s\tremaining: 0us\n",
      "catboost runs for 610.28 seconds.\n",
      "Training Model 4: random forest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 7/8 [2:15:36<20:06, 1206.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest runs for 509.83 seconds.\n",
      "Total running time was 21.96 minutes.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Start training for month34\n",
      "Training Model 0: sgdr\n",
      "SGDRegressor runs for 7.14 seconds.\n",
      "\n",
      "Training Model 1: lightgbm\n",
      "lightgbm runs for 176.09 seconds.\n",
      "\n",
      "Training Model 2: keras\n",
      "Epoch 1/5\n",
      "5245110/5245110 [==============================] - 3s 1us/step - loss: 1.5253 - mean_squared_error: 1.5253\n",
      "Epoch 2/5\n",
      "5245110/5245110 [==============================] - 3s 1us/step - loss: 1.5253 - mean_squared_error: 1.5253\n",
      "Epoch 3/5\n",
      "5245110/5245110 [==============================] - 3s 1us/step - loss: 1.5253 - mean_squared_error: 1.5253\n",
      "Epoch 4/5\n",
      "5245110/5245110 [==============================] - 3s 1us/step - loss: 1.5253 - mean_squared_error: 1.5253\n",
      "Epoch 5/5\n",
      "5245110/5245110 [==============================] - 3s 1us/step - loss: 1.5253 - mean_squared_error: 1.5253\n",
      "214200/214200 [==============================] - 0s 0us/step\n",
      "keras runs for 16.11 seconds.\n",
      "Total running time was 3.35 minutes.\n",
      "Training Model 3: catboost\n",
      "0:\tlearn: 1.2211179\ttotal: 277ms\tremaining: 9m 13s\n",
      "50:\tlearn: 0.9604053\ttotal: 13.6s\tremaining: 8m 39s\n",
      "100:\tlearn: 0.9260022\ttotal: 27.6s\tremaining: 8m 38s\n",
      "150:\tlearn: 0.9152031\ttotal: 42s\tremaining: 8m 34s\n",
      "200:\tlearn: 0.9096075\ttotal: 56.7s\tremaining: 8m 27s\n",
      "250:\tlearn: 0.9043921\ttotal: 1m 11s\tremaining: 8m 21s\n",
      "300:\tlearn: 0.8995935\ttotal: 1m 27s\tremaining: 8m 14s\n",
      "350:\tlearn: 0.8960139\ttotal: 1m 43s\tremaining: 8m 6s\n",
      "400:\tlearn: 0.8929815\ttotal: 1m 58s\tremaining: 7m 53s\n",
      "450:\tlearn: 0.8899923\ttotal: 2m 14s\tremaining: 7m 40s\n",
      "500:\tlearn: 0.8875137\ttotal: 2m 29s\tremaining: 7m 26s\n",
      "550:\tlearn: 0.8850704\ttotal: 2m 44s\tremaining: 7m 11s\n",
      "600:\tlearn: 0.8835338\ttotal: 2m 58s\tremaining: 6m 56s\n",
      "650:\tlearn: 0.8820351\ttotal: 3m 13s\tremaining: 6m 41s\n",
      "700:\tlearn: 0.8805651\ttotal: 3m 28s\tremaining: 6m 26s\n",
      "750:\tlearn: 0.8793282\ttotal: 3m 42s\tremaining: 6m 10s\n",
      "800:\tlearn: 0.8779020\ttotal: 3m 57s\tremaining: 5m 55s\n",
      "850:\tlearn: 0.8764032\ttotal: 4m 12s\tremaining: 5m 40s\n",
      "900:\tlearn: 0.8754156\ttotal: 4m 27s\tremaining: 5m 25s\n",
      "950:\tlearn: 0.8739137\ttotal: 4m 42s\tremaining: 5m 11s\n",
      "1000:\tlearn: 0.8728470\ttotal: 4m 56s\tremaining: 4m 56s\n",
      "1050:\tlearn: 0.8718506\ttotal: 5m 11s\tremaining: 4m 41s\n",
      "1100:\tlearn: 0.8707769\ttotal: 5m 26s\tremaining: 4m 26s\n",
      "1150:\tlearn: 0.8697164\ttotal: 5m 41s\tremaining: 4m 11s\n",
      "1200:\tlearn: 0.8686883\ttotal: 5m 56s\tremaining: 3m 57s\n",
      "1250:\tlearn: 0.8677935\ttotal: 6m 11s\tremaining: 3m 42s\n",
      "1300:\tlearn: 0.8670196\ttotal: 6m 25s\tremaining: 3m 27s\n",
      "1350:\tlearn: 0.8664125\ttotal: 6m 40s\tremaining: 3m 12s\n",
      "1400:\tlearn: 0.8655852\ttotal: 6m 54s\tremaining: 2m 57s\n",
      "1450:\tlearn: 0.8648394\ttotal: 7m 9s\tremaining: 2m 42s\n",
      "1500:\tlearn: 0.8640586\ttotal: 7m 23s\tremaining: 2m 27s\n",
      "1550:\tlearn: 0.8633894\ttotal: 7m 38s\tremaining: 2m 12s\n",
      "1600:\tlearn: 0.8627166\ttotal: 7m 52s\tremaining: 1m 57s\n",
      "1650:\tlearn: 0.8620905\ttotal: 8m 7s\tremaining: 1m 43s\n",
      "1700:\tlearn: 0.8616320\ttotal: 8m 22s\tremaining: 1m 28s\n",
      "1750:\tlearn: 0.8611308\ttotal: 8m 36s\tremaining: 1m 13s\n",
      "1800:\tlearn: 0.8606077\ttotal: 8m 51s\tremaining: 58.7s\n",
      "1850:\tlearn: 0.8600395\ttotal: 9m 5s\tremaining: 43.9s\n",
      "1900:\tlearn: 0.8595881\ttotal: 9m 20s\tremaining: 29.2s\n",
      "1950:\tlearn: 0.8590380\ttotal: 9m 34s\tremaining: 14.4s\n",
      "1999:\tlearn: 0.8585769\ttotal: 9m 49s\tremaining: 0us\n",
      "catboost runs for 607.86 seconds.\n",
      "Training Model 4: random forest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [2:37:33<00:00, 1181.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest runs for 507.86 seconds.\n",
      "Total running time was 21.95 minutes.\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train meta-features M = 15 (12 + 15 = 27)\n",
    "num_first_level_models = 5\n",
    "\n",
    "months_to_generate_meta_features = range(27,last_block +1)\n",
    "\n",
    "mask = dates.isin(months_to_generate_meta_features)\n",
    "\n",
    "target = 'item_cnt_month'\n",
    "\n",
    "y_all_level2 = all_data[target][mask].values\n",
    "X_all_level2 = np.zeros([y_all_level2.shape[0], num_first_level_models])\n",
    "\n",
    "# Now fill X_train_level2 with metafeatures\n",
    "slice_start = 0\n",
    "\n",
    "for cur_block_num in tqdm(months_to_generate_meta_features):\n",
    "\n",
    "    print('-' * 50)\n",
    "    print('Start training for month%d'% cur_block_num)\n",
    "\n",
    "    start_cur_month = time.perf_counter()\n",
    "    # train: all data until current block (excluded), test: current block\n",
    "    cur_X_train_scaled = all_data_scaled.loc[dates <  cur_block_num][feature_columns]\n",
    "    cur_X_test_scaled =  all_data_scaled.loc[dates == cur_block_num][feature_columns]\n",
    "\n",
    "    cur_y_train_scaled = all_data_scaled.loc[dates <  cur_block_num, target].values\n",
    "    cur_y_test_scaled =  all_data_scaled.loc[dates == cur_block_num, target].values\n",
    "    \n",
    "    cur_X_train = all_data.loc[dates <  cur_block_num][feature_columns]\n",
    "    cur_X_test =  all_data.loc[dates == cur_block_num][feature_columns]\n",
    "\n",
    "    cur_y_train = all_data.loc[dates <  cur_block_num, target].values\n",
    "    cur_y_test =  all_data.loc[dates == cur_block_num, target].values\n",
    "\n",
    "    # Create Numpy arrays of train, test and target dataframes to feed into models\n",
    "\n",
    "    train_x_scaled = cur_X_train_scaled.values\n",
    "    train_y_scaled = cur_y_train_scaled.ravel()\n",
    "    test_x_scaled = cur_X_test_scaled.values\n",
    "    test_y_scaled = cur_y_test_scaled.ravel()\n",
    "    \n",
    "    train_x = cur_X_train.values\n",
    "    train_y = cur_y_train.ravel()\n",
    "    test_x = cur_X_test.values\n",
    "    test_y = cur_y_test.ravel()\n",
    "\n",
    "    preds = []\n",
    "\n",
    "    sgdr= SGDRegressor(\n",
    "        penalty = 'l2' ,\n",
    "        random_state = seed )\n",
    "\n",
    "    lgb_params = {\n",
    "                  'feature_fraction': 0.75,\n",
    "                  'metric': 'rmse',\n",
    "                  'nthread':1,\n",
    "                  'min_data_in_leaf': 2**7,\n",
    "                  'bagging_fraction': 0.75,\n",
    "                  'learning_rate': 0.03,\n",
    "                  'objective': 'mse',\n",
    "                  'bagging_seed': 2**7,\n",
    "                  'num_leaves': 2**7,\n",
    "                  'bagging_freq':1,\n",
    "                  'verbose':0\n",
    "                  }\n",
    "    print('Training Model %d: %s'%(len(preds), 'sgdr'))\n",
    "\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    sgdr.fit(train_x_scaled, train_y_scaled)\n",
    "    pred_test = sgdr.predict(test_x_scaled)\n",
    "    preds.append(pred_test)\n",
    "\n",
    "    run = time.perf_counter() - start\n",
    "    print('{} runs for {:.2f} seconds.'.format(sgdr.__class__.__name__, run))\n",
    "    print()\n",
    "\n",
    "    print('Training Model %d: %s'%(len(preds), 'lightgbm'))\n",
    "\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    estimator = lgb.train(lgb_params, lgb.Dataset(train_x_scaled, label=train_y_scaled), 300)\n",
    "    pred_test = estimator.predict(test_x_scaled)\n",
    "    preds.append(pred_test)\n",
    "\n",
    "    run = time.perf_counter() - start\n",
    "\n",
    "    print('{} runs for {:.2f} seconds.'.format('lightgbm', run))\n",
    "    print()\n",
    "\n",
    "    print('Training Model %d: %s'%(len(preds), 'keras'))\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "\n",
    "    def baseline_model():\n",
    "        # create model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(20, input_dim=train_x.shape[1], kernel_initializer='uniform', activation='softplus'))\n",
    "        model.add(Dense(1, kernel_initializer='uniform', activation = 'relu'))\n",
    "\n",
    "        # Compile model\n",
    "        # Nadam = Adam RMSprop with Nesterov momentum.\n",
    "        model.compile(loss='mse', optimizer='Nadam', metrics=['mse'])\n",
    "\n",
    "        return model\n",
    "\n",
    "    estimator = KerasRegressor(build_fn=baseline_model, verbose=1, epochs=5, batch_size = 55000)\n",
    "    estimator.fit(train_x_scaled, train_y_scaled)\n",
    "    pred_test = estimator.predict(test_x_scaled)\n",
    "    preds.append(pred_test)\n",
    "\n",
    "    run = time.perf_counter() - start\n",
    "\n",
    "    print('{} runs for {:.2f} seconds.'.format('keras', run))\n",
    "\n",
    "    cur_month_run_total = time.perf_counter() - start_cur_month\n",
    "\n",
    "    print('Total running time was {:.2f} minutes.'.format(cur_month_run_total/60))\n",
    "    \n",
    "    print('Training Model %d: %s'%(len(preds), 'catboost'))\n",
    "    \n",
    "    start = time.perf_counter()    \n",
    "    #cat_features = [0, 1, 3, 6, 8, 40, 41, 42]\n",
    "    catboost_model = CatBoostRegressor(\n",
    "    iterations=2000,\n",
    "    max_ctr_complexity=4,\n",
    "    random_seed=0,\n",
    "    od_type='Iter',\n",
    "    od_wait=25,\n",
    "    verbose=50,\n",
    "    depth=4\n",
    "    )\n",
    "    \n",
    "    catboost_model.fit(\n",
    "    train_x_scaled, train_y_scaled,\n",
    "    )\n",
    "    \n",
    "    pred_test = catboost_model.predict(test_x_scaled)\n",
    "    preds.append(pred_test)\n",
    "\n",
    "    run = time.perf_counter() - start\n",
    "\n",
    "    print('{} runs for {:.2f} seconds.'.format('catboost', run))\n",
    "    \n",
    "    print('Training Model %d: %s'%(len(preds), 'random forest'))\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=50, max_depth=7, random_state=0, n_jobs=-1)\n",
    "    rf_model.fit(train_x_scaled, train_y_scaled)\n",
    "    \n",
    "    pred_test = rf_model.predict(test_x_scaled)\n",
    "    preds.append(pred_test)\n",
    "\n",
    "    run = time.perf_counter() - start\n",
    "\n",
    "    print('{} runs for {:.2f} seconds.'.format('random forest', run))\n",
    "\n",
    "    cur_month_run_total = time.perf_counter() - start_cur_month\n",
    "\n",
    "    print('Total running time was {:.2f} minutes.'.format(cur_month_run_total/60))\n",
    "\n",
    "    print('-' * 50)\n",
    "\n",
    "    slice_end = slice_start + cur_X_test.shape[0]\n",
    "    X_all_level2[ slice_start : slice_end , :] = np.c_[preds].transpose()\n",
    "    slice_start = slice_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157.57 min: Finish training First level models\n"
     ]
    }
   ],
   "source": [
    "# Split train and test\n",
    "test_nrow = len(preds[0])\n",
    "\n",
    "X_train_level2 = X_all_level2[ : -test_nrow, :]\n",
    "X_test_level2 = X_all_level2[ -test_nrow: , :]\n",
    "y_train_level2 = y_all_level2[ : -test_nrow]\n",
    "y_test_level2 = y_all_level2[ -test_nrow : ]\n",
    "\n",
    "print('%0.2f min: Finish training First level models'%((time.perf_counter() - start_first_level_total)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Second level learning model via linear regression\n",
      "Train R-squared for train_preds_lr_stacking is 1.086452\n"
     ]
    }
   ],
   "source": [
    "pred_list = {}\n",
    "\n",
    "# Second level learning model via linear regression\n",
    "\n",
    "print('Training Second level learning model via linear regression')\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_level2, y_train_level2)\n",
    "\n",
    "# Compute R-squared on the train and test sets.\n",
    "\n",
    "test_preds_lr_stacking = lr.predict(X_test_level2)\n",
    "train_preds_lr_stacking = lr.predict(X_train_level2)\n",
    "\n",
    "print('Train R-squared for %s is %f' %('train_preds_lr_stacking', sqrt(mean_squared_error(y_train_level2, train_preds_lr_stacking))))\n",
    "\n",
    "pred_list['test_preds_lr_stacking'] = test_preds_lr_stacking\n",
    "if Validation:\n",
    "    print('Test R-squared for %s is %f' %('test_preds_lr_stacking', sqrt(mean_squared_error(y_test_level2, test_preds_lr_stacking))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Second level learning model via SGDRegressor\n",
      "Train R-squared for train_preds_lr_stacking is 48607830241593840283272034282433702526976.000000\n",
      "159.51 min: Finish training second level model\n"
     ]
    }
   ],
   "source": [
    "# Second level learning model via SGDRegressor\n",
    "\n",
    "print('Training Second level learning model via SGDRegressor')\n",
    "\n",
    "sgdr= SGDRegressor(\n",
    "    penalty = 'l2' ,\n",
    "    random_state = seed )\n",
    "\n",
    "sgdr.fit(X_train_level2, y_train_level2)\n",
    "\n",
    "test_preds_sgdr_stacking = sgdr.predict(X_test_level2)\n",
    "train_preds_sgdr_stacking = sgdr.predict(X_train_level2)\n",
    "\n",
    "print('Train R-squared for %s is %f' %('train_preds_lr_stacking', sqrt(mean_squared_error(y_train_level2, train_preds_sgdr_stacking))))\n",
    "pred_list['test_preds_sgdr_stacking'] = test_preds_sgdr_stacking\n",
    "\n",
    "if Validation:\n",
    "    print('Test R-squared for %s is %f' %('test_preds_sgdr_stacking', sqrt(mean_squared_error(y_test_level2, test_preds_sgdr_stacking))))\n",
    "\n",
    "print('%0.2f min: Finish training second level model'%((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2748479954636271\n",
      "0.015032679738562092\n",
      "159.52 min: Finish running scripts\n"
     ]
    }
   ],
   "source": [
    "if not Validation:\n",
    "    submission = pd.read_csv('%s/sample_submission.csv' % data_path)\n",
    "    ver = 6\n",
    "    for pred_ver in ['lr_stacking', 'sgdr_stacking']:\n",
    "        print(pred_list['test_preds_' + pred_ver].clip(0,20).mean())\n",
    "        submission['item_cnt_month'] = pred_list['test_preds_' + pred_ver].clip(0,20)\n",
    "        submission[['ID', 'item_cnt_month']].to_csv('%s/ver%d_%s.csv' % (submission_path, ver, pred_ver), index = False)\n",
    "\n",
    "print('%0.2f min: Finish running scripts'%((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
